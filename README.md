# ExecutorTorch Tiled Inference

Memory-efficient CPU inference using ExecutorTorch with a multi-model tiled processing approach.

## Overview

This repository demonstrates how to implement **tiled inference** with ExecutorTorch, handling the framework's static shape requirement through a production-ready multi-model pattern. Perfect for edge and mobile deployment where memory is constrained.

## Key Features

- ‚úÖ **100% Pure ExecutorTorch** - No PyTorch runtime fallback needed
- ‚úÖ **Multi-Model Architecture** - Handles static shapes elegantly
- ‚úÖ **Memory Efficient** - Process large inputs in small chunks
- ‚úÖ **Production Ready** - Real-world pattern for edge deployment
- ‚úÖ **Perfect Accuracy** - Results match non-tiled inference exactly

## Quick Start

### Install Dependencies

```bash
pip install executorch torchvision
```

### Run Demo

```bash
python3 hello_executorch_multimodel.py
```

## How It Works

### The Challenge: Static Shapes

ExecutorTorch models have **fixed input shapes** determined at export time. For tiled inference with variable tile sizes, we need a different approach than traditional frameworks.

### The Solution: Multi-Model Pattern

Export separate models for each expected input size:

```python
models = {
    'full': export_model(1024, 1024),    # Full image model
    'tile': export_model(288, 288),       # Tile model (256 + overlap)
}
```

At runtime:
1. Use **full model** for non-tiled inference
2. Use **tile model** for tiled inference (with padding for boundary tiles)

## Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Export Phase (Offline)             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ PyTorch Model                       ‚îÇ
‚îÇ   ‚Üì                                 ‚îÇ
‚îÇ Export for each size:               ‚îÇ
‚îÇ   ‚Ä¢ Full: 1024√ó1024 ‚Üí .pte         ‚îÇ
‚îÇ   ‚Ä¢ Tile: 288√ó288 ‚Üí .pte           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Runtime Phase (On-Device)          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Non-tiled: Use full model          ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ Tiled:                              ‚îÇ
‚îÇ   1. Load tile model                ‚îÇ
‚îÇ   2. For each tile:                 ‚îÇ
‚îÇ      ‚Ä¢ Extract with overlap         ‚îÇ
‚îÇ      ‚Ä¢ Pad to match model size      ‚îÇ
‚îÇ      ‚Ä¢ Process with tile model      ‚îÇ
‚îÇ      ‚Ä¢ Stitch into output           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Test Results

```
Non-Tiled Inference (ExecutorTorch):
  - Input: 1024√ó1024 (4 MB)
  - Processing time: 43.35 ms
  
Tiled Inference (ExecutorTorch - 16 tiles):
  - Tile size: 256√ó256 + 16px overlap
  - Processing time: 87.06 ms
  - Accuracy: Perfect (0.00e+00 difference)
```

## Benefits of Tiled Processing

- ‚úÖ **Reduced Memory Usage** - Process large inputs in manageable chunks
- ‚úÖ **Better Cache Locality** - Smaller working sets fit in CPU cache
- ‚úÖ **Scalable** - Handle inputs larger than available memory
- ‚úÖ **Edge Optimized** - Ideal for memory-constrained devices

## Use Cases

### When to Use Tiled Inference

- ‚úÖ Processing very large images (>2K resolution)
- ‚úÖ Memory-constrained devices (mobile, embedded)  
- ‚úÖ Avoiding Out-Of-Memory errors
- ‚úÖ Batch processing large datasets

### Real-World Applications

- üì± Mobile image processing apps
- ü§ñ Edge AI devices
- üì∑ High-resolution image analysis
- üè• Medical imaging on embedded systems
- üõ∞Ô∏è Satellite image processing

## Implementation Details

### File Structure

- `hello_executorch_multimodel.py` - Main demo showcasing multi-model tiled inference
- `.gitignore` - Configured to ignore generated `.pte` model files
- `README.md` - This documentation

### Code Highlights

**Model Export:**
```python
def export_executorch_model(model, h, w, filename):
    exported = torch.export.export(model, (example_input,))
    edge_program = to_edge(exported)
    et_program = edge_program.to_executorch()
    
    with open(filename, "wb") as f:
        f.write(et_program.buffer)
```

**Tiled Inference with Padding:**
```python
# Extract tile
tile = input_data[:, :, start_h:end_h, start_w:end_w].contiguous()

# Pad to expected size for boundary tiles
if tile.shape != expected_shape:
    padded_tile = torch.zeros(expected_shape)
    padded_tile[:, :, :actual_h, :actual_w] = tile
    tile = padded_tile

# Process with ExecutorTorch
result = tile_model.run_method("forward", (tile,))
```

## Environment

- **Python**: 3.10
- **PyTorch**: 2.8.0+cu128
- **ExecutorTorch**: 0.7.0
- **System**: Ubuntu 22.04

## Resources

- [ExecutorTorch Documentation](https://pytorch.org/executorch/)
- [PyTorch Edge](https://pytorch.org/blog/pytorch-edge/)
- [ExecutorTorch GitHub](https://github.com/pytorch/executorch)

## License

MIT License - See individual dependencies for their licenses.

## Contributing

Feel free to open issues or submit PRs for improvements!

---

**Note**: This pattern is used in real mobile/embedded applications where ExecutorTorch's static shapes require pre-compiled models for each expected input configuration.
