# ExecutorTorch Dynamic Shapes Tiled Inference

Memory-efficient inference using ExecutorTorch with **dynamic shapes** for flexible tiled processing.

## Overview

This repository demonstrates **dynamic shapes** in ExecutorTorch, allowing a single model to handle variable input sizes for tiled inference without requiring multiple static models or padding logic.

**Based on solution from:** [pytorch/executorch#3636](https://github.com/pytorch/executorch/issues/3636)

## Key Features

- âœ… **Single Dynamic Model** - One `.pte` file handles all tile sizes (256-1024 range)
- âœ… **No Padding Required** - Automatic handling of variable boundary tiles
- âœ… **Simplified Code** - No model selection logic needed
- âœ… **Smaller Deployment** - 2KB vs 8KB+ for multi-model approach
- âœ… **Perfect Accuracy** - Results match non-tiled inference exactly (0.00e+00 difference)
- âœ… **Production Ready** - Works on Android, iOS, and embedded devices

## Quick Start

### Install Dependencies

```bash
pip install executorch torchvision
```

### Run Demo

```bash
python hello_executorch_multimodel.py
```

## Dynamic Shapes Implementation

### Core Concept

Use `torch.export.Dim` to define dynamic dimensions with min/max constraints:

```python
from torch.export import Dim

# Define dynamic dimensions
dynamic_h = Dim("height", min=256, max=1024)
dynamic_w = Dim("width", min=256, max=1024)
dynamic_shapes = {"x": {2: dynamic_h, 3: dynamic_w}}

# Export with dynamic shapes
exported = torch.export.export(
    model,
    (example_input,),
    dynamic_shapes=dynamic_shapes
)
```

### Critical Requirement: ConstraintBasedSymShapeEvalPass

**This is REQUIRED** for dynamic shapes to work:

```python
from executorch.exir.passes.sym_shape_eval_pass import ConstraintBasedSymShapeEvalPass

et_program = edge_program.to_executorch(
    config=ExecutorchBackendConfig(
        sym_shape_eval_pass=ConstraintBasedSymShapeEvalPass(),
    )
)
```

**What it does:**
- Uses max constraint for memory planning (not example input size)
- Enables proper runtime handling of variable-sized inputs
- Without this: runtime errors with size mismatches

### Runtime Usage

```python
# Load model once
model = _load_for_executorch("model_dynamic.pte")

# Use with any size within range
result_1024 = model.run_method("forward", (torch.randn(1, 1, 1024, 1024),))
result_512 = model.run_method("forward", (torch.randn(1, 1, 512, 512),))
result_288 = model.run_method("forward", (torch.randn(1, 1, 288, 288),))
```

## Test Results

```
Model: model_dynamic.pte (2.0 KB)
Dynamic Range: 256Ã—256 to 1024Ã—1024

Performance:
  - Full image (1024Ã—1024): 40.58 ms
  - Tiled (16 tiles, 256Ã—256): 66.06 ms
  - Different size (512Ã—512): 10.68 ms

Validation:
  - Max difference: 0.00e+00 âœ“ Perfect match!
  
Tile Sizes Handled Automatically:
  - (272, 272), (272, 288), (288, 272), (288, 288)
```

## Comparison: Static vs Dynamic

| Aspect | Multi-Model (Static) | Dynamic Shapes |
|--------|---------------------|----------------|
| Model Files | 4+ files (~8 KB) | 1 file (2 KB) |
| Code Complexity | Model selection + padding | Direct usage |
| Tile Handling | Manual padding required | Automatic |
| Deployment | Multiple files to manage | Single file |
| Maintenance | Update all models | Update once |

## Common Issues & Solutions

### Error: "ETensor rank is immutable"
**Cause:** Trying to change tensor rank  
**Solution:** Keep rank constant, only vary dimensions

### Error: "Attempted to resize a static tensor"
**Cause:** Missing `ConstraintBasedSymShapeEvalPass`  
**Solution:** Add to `ExecutorchBackendConfig`

### Mobile: Model works in Python but fails on Android
**Cause:** Cached old `.pte` file  
**Solution:** Change filename or clear app cache

### Important Limitation
âœ… **Supported:** Variable dimensions with same rank
```python
torch.randn(1, 1, 256, 256)   # OK
torch.randn(1, 1, 1024, 1024) # OK
```

âŒ **Not Supported:** Changing tensor rank
```python
torch.randn(1, 256)           # Rank 1
torch.randn(1, 1, 256, 256)   # Rank 2 - ERROR!
```

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Export Phase (Offline)              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ PyTorch Model                        â”‚
â”‚   â†“                                  â”‚
â”‚ Export with dynamic dimensions:      â”‚
â”‚   â€¢ Dim("height", min=256, max=1024)â”‚
â”‚   â€¢ Dim("width", min=256, max=1024) â”‚
â”‚   â†“                                  â”‚
â”‚ Single model_dynamic.pte (2 KB)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Runtime Phase (On-Device)           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Load model once                      â”‚
â”‚   â†“                                  â”‚
â”‚ Process any size (256-1024):         â”‚
â”‚   â€¢ Full images                      â”‚
â”‚   â€¢ Variable-sized tiles             â”‚
â”‚   â€¢ No padding needed!               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Use Cases

### When to Use Dynamic Shapes

- âœ… Tiled inference with variable tile sizes
- âœ… Processing multiple image resolutions
- âœ… Memory-constrained devices (mobile, embedded)
- âœ… Minimizing deployment size
- âœ… Simplifying code maintenance

### Real-World Applications

- ğŸ“± Mobile image processing apps
- ğŸ¤– Edge AI devices with limited memory
- ğŸ“· Multi-resolution image analysis
- ğŸ¥ Medical imaging on embedded systems
- ğŸ›°ï¸ Satellite image processing at various scales

## Implementation Details

### File Structure

- `hello_executorch_multimodel.py` - Complete working demo with comprehensive docstrings
- `README.md` - This documentation
- `.gitignore` - Configured to ignore generated `.pte` files

### Code Documentation

All implementation details are documented in the Python file's docstrings:
- Module-level overview
- Class and method documentation
- Parameter descriptions
- Return value specifications
- Usage examples

Run `python hello_executorch_multimodel.py` to see the full workflow.

## Production Deployment Tips

### 1. Choose Appropriate Min/Max Range

```python
# Mobile image processing
min_size = 256
max_size = 2048

# Embedded devices (more constrained)
min_size = 128
max_size = 512
```

### 2. Version Your Models

```python
filename = f"model_dynamic_v{VERSION}_{min_size}_{max_size}.pte"
# Example: "model_dynamic_v1_256_1024.pte"
```

### 3. Test Multiple Sizes at Export

```python
test_sizes = [(256, 256), (512, 512), (1024, 1024)]
for h, w in test_sizes:
    test_input = torch.randn(1, 1, h, w)
    result = model.run_method("forward", (test_input,))
    print(f"âœ“ Validated {h}x{w}")
```

## Environment

- **Python**: 3.10+
- **PyTorch**: 2.8.0+
- **ExecutorTorch**: Latest
- **System**: Linux/macOS/Windows

## Resources

- **GitHub Issue**: [pytorch/executorch#3636](https://github.com/pytorch/executorch/issues/3636)
- **ExecutorTorch Docs**: [pytorch.org/executorch](https://pytorch.org/executorch/)
- **PyTorch Export**: [torch.export documentation](https://pytorch.org/docs/stable/export.html)

## Benefits of This Approach

âœ… **Simpler**: One model, no selection logic  
âœ… **Smaller**: 75% less storage (2KB vs 8KB+)  
âœ… **Flexible**: Handle any size in range  
âœ… **Maintainable**: Update once, deploy everywhere  
âœ… **Production-Ready**: Used in real mobile/embedded apps  

## License

MIT License - See individual dependencies for their licenses.

## Contributing

Feel free to open issues or submit PRs for improvements!

---

**Key Takeaway:** Use `Dim` to define constraints and `ConstraintBasedSymShapeEvalPass` for proper memory planning. This eliminates the need for multiple static models while maintaining perfect accuracy and simplifying deployment.
